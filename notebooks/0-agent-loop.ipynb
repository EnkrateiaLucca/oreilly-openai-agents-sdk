{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Agent Loop\n",
    "\n",
    "The most important concept in this entire course. Everything else â€” tools, handoffs, guardrails â€” is just decoration on top of this one idea.\n",
    "\n",
    "An agent is a **while loop with an LLM as the control flow decision-maker**.\n",
    "\n",
    "![](2026-02-14-14-06-49.png)\n",
    "\n",
    "The loop: user message â†’ LLM decides â†’ either **respond** (exit) or **call a tool** â†’ execute tool â†’ feed result back â†’ LLM decides again â†’ repeat.\n",
    "\n",
    "The key insight: **the model decides when to stop**. That's what makes it an agent, not a chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup â€” run once\n",
    "# pip install openai-agents\n",
    "\n",
    "import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: The Simplest Agent â€” One Iteration\n",
    "\n",
    "No tools. The loop runs exactly **once**: message in â†’ LLM responds â†’ done.\n",
    "\n",
    "This is the degenerate case. The agent has nothing to do except think and respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Portugal is Lisbon.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from agents import Agent, Runner\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Simple Agent\",\n",
    "    instructions=\"You are a helpful assistant. Be concise.\",\n",
    ")\n",
    "\n",
    "result = Runner.run_sync(agent, \"What is the capital of Portugal?\")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. Five lines. The `Runner` executed the agent loop, but since there are no tools, the LLM had no reason to loop â€” it just answered.\n",
    "\n",
    "Let's peek at what actually happened inside the loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse â†’ [ResponseOutputMessage(id='msg_0dd0b550dbc9d6f300699081b23a388197897d3492f92555b1', content=[ResponseOutputText(annotations=[], text='The capital of Portugal is Lisbon.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the raw items the Runner produced\n",
    "for item in result.raw_responses:\n",
    "    print(type(item).__name__, \"â†’\", getattr(item, 'output', None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One response. One iteration. The model decided immediately: \"I can answer this without any tools.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Demo 2: Add One Tool â€” Watch the Loop Iterate\n",
    "\n",
    "Now we give the agent a tool. The loop should run **twice**:\n",
    "1. LLM decides to call the tool\n",
    "2. Tool executes, result fed back â†’ LLM decides to respond\n",
    "\n",
    "We'll add a print inside the tool so you can *see* the loop in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending message to agent...\n",
      "\n",
      "  ðŸ”§ [Tool called] get_temperature('Lisbon')\n",
      "\n",
      "ðŸ’¬ Final output: The current temperature in Lisbon is 22Â°C.\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, function_tool\n",
    "\n",
    "@function_tool\n",
    "def get_temperature(city: str) -> str:\n",
    "    \"\"\"Get the current temperature for a city.\"\"\"\n",
    "    print(f\"  ðŸ”§ [Tool called] get_temperature('{city}')\")\n",
    "    # Fake data â€” in production this would hit a weather API\n",
    "    temperatures = {\"lisbon\": \"22Â°C\", \"tokyo\": \"18Â°C\", \"new york\": \"5Â°C\"}\n",
    "    return temperatures.get(city.lower(), \"Unknown city\")\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Weather Agent\",\n",
    "    instructions=\"You help users check the weather. Be concise.\",\n",
    "    tools=[get_temperature],\n",
    ")\n",
    "\n",
    "print(\"Sending message to agent...\\n\")\n",
    "result = Runner.run_sync(agent, \"What's the temperature in Lisbon?\")\n",
    "print(f\"\\nðŸ’¬ Final output: {result.final_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the sequence:\n",
    "1. We sent the message\n",
    "2. The LLM **decided** to call `get_temperature` (not us â€” the model chose this)\n",
    "3. The tool ran and returned a result\n",
    "4. The LLM got the result back, **decided** it had enough information, and responded\n",
    "\n",
    "Two iterations. The model decided both *what to do* and *when to stop*.\n",
    "\n",
    "---\n",
    "\n",
    "## Demo 3: Multi-Step Reasoning â€” The Loop Plans\n",
    "\n",
    "Give the agent a task that requires **two tools called in sequence**. Now we'll see three iterations â€” and the model *planning* its approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending message to agent...\n",
      "\n",
      "  ðŸ”§ [Tool called] get_temperature_celsius('Lisbon')\n",
      "  ðŸ”§ [Tool called] celsius_to_fahrenheit('22')\n",
      "\n",
      "ðŸ’¬ Final output: The temperature in Lisbon is 71.6Â°F.\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, function_tool\n",
    "\n",
    "@function_tool\n",
    "def get_temperature_celsius(city: str) -> str:\n",
    "    \"\"\"Get the current temperature in Celsius for a city.\"\"\"\n",
    "    print(f\"  ðŸ”§ [Tool called] get_temperature_celsius('{city}')\")\n",
    "    temperatures = {\"lisbon\": \"22\", \"tokyo\": \"18\", \"new york\": \"5\"}\n",
    "    return temperatures.get(city.lower(), \"Unknown city\")\n",
    "\n",
    "@function_tool\n",
    "def celsius_to_fahrenheit(celsius: str) -> str:\n",
    "    \"\"\"Convert a Celsius temperature to Fahrenheit.\"\"\"\n",
    "    print(f\"  ðŸ”§ [Tool called] celsius_to_fahrenheit('{celsius}')\")\n",
    "    f = round(float(celsius) * 9/5 + 32, 1)\n",
    "    return str(f)\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Weather Agent v2\",\n",
    "    instructions=\"You help users check the weather. Be concise.\",\n",
    "    tools=[get_temperature_celsius, celsius_to_fahrenheit],\n",
    ")\n",
    "\n",
    "print(\"Sending message to agent...\\n\")\n",
    "result = Runner.run_sync(agent, \"What's the temperature in Lisbon in Fahrenheit?\")\n",
    "print(f\"\\nðŸ’¬ Final output: {result.final_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three iterations. The model:\n",
    "1. Called `get_temperature_celsius` first (it needs the raw number)\n",
    "2. Took that result, called `celsius_to_fahrenheit` (sequential reasoning)\n",
    "3. Got both results, synthesized a final answer\n",
    "\n",
    "Nobody told it the order. **The LLM figured out the dependency chain on its own.** That's the agent loop doing its job.\n",
    "\n",
    "---\n",
    "\n",
    "## Demo 4: Inspecting the Conversation Between Iterations\n",
    "\n",
    "What does the \"conversation\" actually look like between loop iterations? Let's inspect `result.to_input_list()` â€” this is the raw message history the Runner built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] ðŸ‘¤ USER: What's the temperature in Lisbon in Fahrenheit?\n",
      "[1] unknown: {\n",
      "  \"arguments\": \"{\\\"city\\\":\\\"Lisbon\\\"}\",\n",
      "  \"call_id\": \"call_fYYORL97tJYXOt0RC0UWWf05\",\n",
      "  \"name\": \"get_temperature_celsius\",\n",
      "  \"type\": \"function_call\",\n",
      "  \"id\": \"fc_05a7ac8d8067a05600699081f310fc8190af\n",
      "[2] unknown: {\n",
      "  \"call_id\": \"call_fYYORL97tJYXOt0RC0UWWf05\",\n",
      "  \"output\": \"22\",\n",
      "  \"type\": \"function_call_output\"\n",
      "}\n",
      "[3] unknown: {\n",
      "  \"arguments\": \"{\\\"celsius\\\":\\\"22\\\"}\",\n",
      "  \"call_id\": \"call_zjUhXt1AfPOabbSuij5UcJ53\",\n",
      "  \"name\": \"celsius_to_fahrenheit\",\n",
      "  \"type\": \"function_call\",\n",
      "  \"id\": \"fc_05a7ac8d8067a05600699081f494e08190ad024\n",
      "[4] unknown: {\n",
      "  \"call_id\": \"call_zjUhXt1AfPOabbSuij5UcJ53\",\n",
      "  \"output\": \"71.6\",\n",
      "  \"type\": \"function_call_output\"\n",
      "}\n",
      "[5] ðŸ¤– ASSISTANT: [{'annotations': [], 'text': 'The temperature in Lisbon is 71.6Â°F.', 'type': 'output_text', 'logprobs': []}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Show the full message history the loop produced\n",
    "for i, item in enumerate(result.to_input_list()):\n",
    "    role = item.get(\"role\", \"unknown\")\n",
    "    \n",
    "    if role == \"user\":\n",
    "        print(f\"[{i}] ðŸ‘¤ USER: {item['content']}\")\n",
    "    elif role == \"assistant\":\n",
    "        content = item.get(\"content\")\n",
    "        tool_calls = item.get(\"tool_calls\", [])\n",
    "        if content:\n",
    "            print(f\"[{i}] ðŸ¤– ASSISTANT: {content}\")\n",
    "        if tool_calls:\n",
    "            for tc in tool_calls:\n",
    "                print(f\"[{i}] ðŸ¤– ASSISTANT â†’ tool_call: {tc}\")\n",
    "    elif role == \"tool\":\n",
    "        print(f\"[{i}] ðŸ”§ TOOL RESULT: {item.get('content', item)}\")\n",
    "    else:\n",
    "        print(f\"[{i}] {role}: {json.dumps(item, indent=2, default=str)[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what's actually happening inside the `Runner`. Each iteration appends to this list â€” tool calls, tool results, and finally the assistant's response. The LLM sees the *entire history* each time it's called, which is how it knows what it already did and what's left to do.\n",
    "\n",
    "---\n",
    "\n",
    "## Demo 5: When the Loop Goes Wrong\n",
    "\n",
    "The agent loop is a while loop. While loops can spin. Let's see what happens when we give the agent a task it **can't solve** with its available tools â€” and protect ourselves with `max_turns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking about a city the tool doesn't know...\n",
      "\n",
      "  ðŸ”§ [Tool called] get_temperature_celsius('Reykjavik')\n",
      "\n",
      "ðŸ’¬ Final output: I couldn't retrieve the temperature for Reykjavik. The system seems to have trouble recognizing the city. You might want to try checking a weather website or app.\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, function_tool\n",
    "\n",
    "@function_tool\n",
    "def get_temperature_celsius(city: str) -> str:\n",
    "    \"\"\"Get the current temperature in Celsius for a city.\"\"\"\n",
    "    print(f\"  ðŸ”§ [Tool called] get_temperature_celsius('{city}')\")\n",
    "    # This tool only knows about 3 cities\n",
    "    temperatures = {\"lisbon\": \"22\", \"tokyo\": \"18\", \"new york\": \"5\"}\n",
    "    return temperatures.get(city.lower(), \"Unknown city\")\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Limited Agent\",\n",
    "    instructions=\"You help users check the weather. Always use your tools to look up temperatures.\",\n",
    "    tools=[get_temperature_celsius],\n",
    ")\n",
    "\n",
    "print(\"Asking about a city the tool doesn't know...\\n\")\n",
    "result = Runner.run_sync(\n",
    "    agent,\n",
    "    \"What's the temperature in Reykjavik?\",\n",
    "    max_turns=5  # Safety net: stop after 5 iterations\n",
    ")\n",
    "print(f\"\\nðŸ’¬ Final output: {result.final_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model might handle this gracefully (\"I don't have data for Reykjavik\") â€” or it might try calling the tool multiple times hoping for a different result. The behavior depends on the instructions and the model's judgment.\n",
    "\n",
    "**This is why `max_turns` exists.** It's a circuit breaker for the agent loop. In production, you always set it.\n",
    "\n",
    "---\n",
    "\n",
    "## Demo 6: The Loop Without the SDK\n",
    "\n",
    "To really understand what `Runner.run_sync` is doing, here's the same logic written as a raw while loop. **This is not how you'd write production code** â€” but it demystifies the abstraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loop iteration 1 ---\n",
      "Model called: get_temperature({'city': 'Lisbon'})\n",
      "Tool returned: 22Â°C\n",
      "--- Loop iteration 2 ---\n",
      "Model responded: The temperature in Lisbon is 22Â°C.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# The same tool, defined manually\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_temperature\",\n",
    "        \"description\": \"Get the current temperature for a city.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"city\": {\"type\": \"string\"}},\n",
    "            \"required\": [\"city\"]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "def execute_tool(name, args):\n",
    "    if name == \"get_temperature\":\n",
    "        temps = {\"lisbon\": \"22Â°C\", \"tokyo\": \"18Â°C\"}\n",
    "        return temps.get(args[\"city\"].lower(), \"Unknown\")\n",
    "\n",
    "# The agent loop â€” written explicitly\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You help with weather. Be concise.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the temperature in Lisbon?\"}\n",
    "]\n",
    "\n",
    "turn = 0\n",
    "while turn < 10:  # max_turns safety\n",
    "    turn += 1\n",
    "    print(f\"--- Loop iteration {turn} ---\")\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "    )\n",
    "    \n",
    "    message = response.choices[0].message\n",
    "    messages.append(message)\n",
    "    \n",
    "    # EXIT CONDITION: no tool calls â†’ the model decided to respond\n",
    "    if not message.tool_calls:\n",
    "        print(f\"Model responded: {message.content}\")\n",
    "        break\n",
    "    \n",
    "    # CONTINUE: execute tool calls, feed results back\n",
    "    for tc in message.tool_calls:\n",
    "        args = json.loads(tc.function.arguments)\n",
    "        print(f\"Model called: {tc.function.name}({args})\")\n",
    "        result = execute_tool(tc.function.name, args)\n",
    "        print(f\"Tool returned: {result}\")\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": tc.id,\n",
    "            \"content\": result\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That's the entire agent loop.** A while loop, an LLM call, a branch: did the model call a tool or not? The OpenAI Agents SDK wraps this in `Runner` and adds error handling, tracing, guardrails, and handoff support â€” but the core is exactly this.\n",
    "\n",
    "---\n",
    "\n",
    "## Takeaways\n",
    "\n",
    "- An agent is a **loop**, not a single LLM call\n",
    "- The **model** decides what to do next (call a tool or respond)\n",
    "- The **model** decides when to stop (no hardcoded exit condition)\n",
    "- `Runner.run_sync` is doing the loop for you â€” with `max_turns` as a safety net\n",
    "- The conversation history grows with each iteration â€” the model sees everything it already did\n",
    "\n",
    "Next up: we'll go deeper on **tools** â€” the things that give the agent actual capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
